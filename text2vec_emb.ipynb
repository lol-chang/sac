{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4abdcbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 텍스트 개수: 2733\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = r\"C:\\Users\\changjin\\workspace\\lab\\pln\\data_set\\null_X\"\n",
    "files = [\"attractions_fixed.csv\", \"restaurants_fixed.csv\", \"accommodations_fixed.csv\"]\n",
    "\n",
    "texts = []\n",
    "\n",
    "for fname in files:\n",
    "    df = pd.read_csv(os.path.join(path, fname))\n",
    "    for col in [\"like\", \"dislike\"]:\n",
    "        if col in df.columns:\n",
    "            texts.extend(df[col].dropna().tolist())\n",
    "\n",
    "print(f\"총 텍스트 개수: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07776830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (5.1.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.56.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (2.9.0.dev20250707+cu128)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (1.16.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (0.35.3)\n",
      "Requirement already satisfied: Pillow in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sentence-transformers) (4.14.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\changjin\\appdata\\roaming\\python\\python311\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.5.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\changjin\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\changjin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bed57ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\changjin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\changjin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\changjin\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 768])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 모델 로드\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# 샘플 키워드\n",
    "keywords = [\n",
    "    \"Scenic views\",\n",
    "    \"Delicious food\",\n",
    "    \"Comfortable bedding\",\n",
    "    \"Historical temples\",\n",
    "    \"Street food\",\n",
    "    \"Pet-friendly\"\n",
    "]\n",
    "\n",
    "# 임베딩 변환\n",
    "embeddings = model.encode(keywords, convert_to_tensor=True)\n",
    "\n",
    "print(embeddings.shape)  # (6, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65e84ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 2: ['Scenic views']\n",
      "Cluster 1: ['Delicious food', 'Street food']\n",
      "Cluster 0: ['Comfortable bedding']\n",
      "Cluster 3: ['Historical temples']\n",
      "Cluster 4: ['Pet-friendly']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 예: 5개 클러스터\n",
    "num_clusters = 5\n",
    "clustering_model = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "clustering_model.fit(embeddings.cpu().numpy())\n",
    "\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "# 클러스터별 키워드 묶기\n",
    "clusters = {}\n",
    "for i, label in enumerate(cluster_assignment):\n",
    "    clusters.setdefault(label, []).append(keywords[i])\n",
    "\n",
    "for label, cluster_keywords in clusters.items():\n",
    "    print(f\"Cluster {label}: {cluster_keywords}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff977794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned Like: ['Scenic beauty', 'Quiet atmosphere', 'Cleanliness', 'Walking trails', 'Snow activities', 'Well-designed park', 'Walking paths', 'Clean environment', 'Attractive sculptures', 'Scenic beauty']\n",
      "✅ Cleaned Dislike: ['Lack of microwave', 'Bring own towels', 'Disabled parking misuse', 'Closed restrooms', 'Limited attractions', 'Narrow paths', 'Challenging hike', 'Limited visibility', 'Lack of washbasins', 'Insects presence']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 122/122 [00:00<00:00, 126.34it/s]\n",
      "Batches: 100%|██████████| 44/44 [00:00<00:00, 130.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like embedding shape: torch.Size([7776, 768])\n",
      "Dislike embedding shape: torch.Size([2796, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 경로\n",
    "path = r\"C:\\Users\\changjin\\workspace\\lab\\pln\\data_set\\null_X\"\n",
    "files = [\"attractions_fixed.csv\", \"restaurants_fixed.csv\", \"accommodations_fixed.csv\"]\n",
    "\n",
    "# 저장소\n",
    "like_keywords, dislike_keywords = [], []\n",
    "\n",
    "# 파일 로드 및 키워드 추출\n",
    "for fname in files:\n",
    "    df = pd.read_csv(os.path.join(path, fname))\n",
    "    for col, storage in [(\"like\", like_keywords), (\"dislike\", dislike_keywords)]:\n",
    "        if col in df.columns:\n",
    "            storage.extend(df[col].dropna().tolist())\n",
    "\n",
    "# --- 1. 세미콜론 분리 ---\n",
    "def split_keywords(keyword_list):\n",
    "    clean = []\n",
    "    for kw in keyword_list:\n",
    "        clean.extend([x.strip() for x in kw.split(\";\") if x.strip()])\n",
    "    return clean\n",
    "\n",
    "clean_like = split_keywords(like_keywords)\n",
    "clean_dislike = split_keywords(dislike_keywords)\n",
    "\n",
    "print(\"✅ Cleaned Like:\", clean_like[:10])\n",
    "print(\"✅ Cleaned Dislike:\", clean_dislike[:10])\n",
    "\n",
    "# --- 2. 임베딩 ---\n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "like_embeddings = model.encode(clean_like, batch_size=64, convert_to_tensor=True, show_progress_bar=True)\n",
    "dislike_embeddings = model.encode(clean_dislike, batch_size=64, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Like embedding shape:\", like_embeddings.shape)\n",
    "print(\"Dislike embedding shape:\", dislike_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0b6a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Like Keywords Clustering ===\n",
      "Cluster 7: ['Scenic view', 'Scenic views', 'Beautiful view', 'Ocean view', 'Scenic beauty']\n",
      "Cluster 5: ['Pleasant atmosphere', 'Spacious room', 'Comfortable bedding', 'Quiet atmosphere', 'Cozy atmosphere']\n",
      "Cluster 1: ['Cleanliness', 'Clean environment', 'Clean facilities', 'Clean interior', 'Clean room']\n",
      "Cluster 4: ['Family-friendly', 'Convenient amenities', 'Good facilities', 'Well-maintained facilities', 'Well-equipped amenities']\n",
      "Cluster 2: ['Friendly service', 'Friendly staff', 'Quick service', 'Attentive service', 'Excellent service']\n",
      "Cluster 9: ['Variety of dishes', 'Flavorful broth', 'Diverse menu', 'Authentic taste', 'Spicy flavor']\n",
      "Cluster 8: ['Fresh ingredients', 'Fresh seafood', 'High-quality ingredients', 'High-quality meat', 'Tender meat']\n",
      "Cluster 6: ['Delicious food', 'Tasty food', 'Delicious taste', 'Delicious desserts', 'Delicious coffee']\n",
      "Cluster 0: ['Good value', 'Affordable prices', 'Affordable price', 'Affordable pricing', 'Value for money']\n",
      "Cluster 3: ['Generous portions', 'Satisfying portions', 'Generous portion', 'Generous side dishes', 'Large portions']\n",
      "\n",
      "=== Dislike Keywords Clustering ===\n",
      "Cluster 0: ['Limited menu', 'Poor food quality', 'Limited menu options', 'Strong seasoning', 'Limited side dishes']\n",
      "Cluster 1: ['Poor cleanliness', 'Unpleasant odor', 'Weak water pressure', 'Poor hygiene', 'Hygiene issues']\n",
      "Cluster 3: ['Outdated facilities', 'Lack of privacy', 'Lack of amenities', 'Limited seating', 'Lack of elevator']\n",
      "Cluster 2: ['Poor soundproofing', 'Noise', 'Noise issues', 'Poor maintenance', 'Insect presence']\n",
      "Cluster 5: ['Unfriendly service', 'Poor service', 'Slow service', 'Unfriendly staff', 'Long wait']\n",
      "Cluster 4: ['High price', 'Poor value', 'Overpriced', 'High prices', 'Additional charges']\n",
      "Cluster 6: ['Small portions', 'Small room size', 'Small room', 'Small bathroom', 'Small portion size']\n",
      "Cluster 7: ['Bland taste', 'Average taste', 'Salty food', 'Unpleasant taste', 'Poor taste']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def cluster_keywords(keywords, embeddings, n_clusters=10, top_n=5, label=\"Like\"):\n",
    "    \"\"\"\n",
    "    KMeans로 키워드 클러스터링 후 대표 라벨 추출\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== {label} Keywords Clustering ===\")\n",
    "    \n",
    "    # KMeans 클러스터링\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "    clusters = {}\n",
    "    for i, kw in enumerate(keywords):\n",
    "        cluster_id = labels[i]\n",
    "        clusters.setdefault(cluster_id, []).append(kw)\n",
    "\n",
    "    # 각 클러스터별 대표 라벨 추출\n",
    "    for cluster_id, kws in clusters.items():\n",
    "        counter = Counter(kws)\n",
    "        top_keywords = [kw for kw, _ in counter.most_common(top_n)]\n",
    "        print(f\"Cluster {cluster_id}: {top_keywords[:top_n]}\")\n",
    "\n",
    "    return clusters\n",
    "\n",
    "# 👉 사용 예시 (like / dislike 나눠서)\n",
    "like_clusters = cluster_keywords(clean_like, model.encode(clean_like, convert_to_tensor=False), \n",
    "                                 n_clusters=10, top_n=5, label=\"Like\")\n",
    "\n",
    "dislike_clusters = cluster_keywords(clean_dislike, model.encode(clean_dislike, convert_to_tensor=False), \n",
    "                                    n_clusters=8, top_n=5, label=\"Dislike\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390bbfee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
