import os
import json
import numpy as np
import pandas as pd
import re
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from openai import OpenAI
import weaviate
from weaviate.auth import AuthApiKey
from weaviate.classes import query as wq
from tqdm import tqdm  # ğŸ”¹ ì§„í–‰ë¥  í‘œì‹œ

# ========== CONFIG ==========
CONFIG = {
    "USER_FILE": r"C:\\Users\\changjin\\workspace\\lab\\pln\\data_set\\1_user_info.csv",
    "DATA_DIR": r"C:\\Users\\changjin\\workspace\\lab\\pln\\data_set\\null_X",
    "OUTPUT_DIR": r"C:\\Users\\changjin\\workspace\\lab\\pln\\vector_embedding\\review_count_with_llm\\user_results",
    "TOP_K": 30
}

CATEGORY_FILES = {
    "Accommodation": "accommodations_fixed.csv",
    "ì¹´í˜": "cafe_fixed.csv",
    "ìŒì‹ì ": "restaurants_fixed.csv",
    "ê´€ê´‘ì§€": "attractions_fixed.csv"
}

CATEGORY_TRANSLATE = {
    "Accommodation": "Accommodation",
    "ì¹´í˜": "Cafe",
    "ìŒì‹ì ": "Restaurant",
    "ê´€ê´‘ì§€": "Attraction"
}

# ========== 1. í™˜ê²½ ë³€ìˆ˜ ë° í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ==========
load_dotenv()
api_key = os.getenv("WEAVIATE_API_KEY")
cluster_url = os.getenv("WEAVIATE_CLUSTER_URL")
openai_key = os.getenv("OPENAI_API_KEY")

client = weaviate.connect_to_weaviate_cloud(
    cluster_url=cluster_url,
    auth_credentials=AuthApiKey(api_key)
)
collection = client.collections.get("Place")
openai_client = OpenAI(api_key=openai_key)

print("âœ… Weaviate ë° OpenAI í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì™„ë£Œ\n")

# ========== 2. ëª¨ë¸ ë¡œë“œ ==========
print("ğŸ”§ SentenceTransformer ëª¨ë¸ ë¡œë”© ì¤‘...")
model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")
print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ\n")


# ========== 3. í›„ë³´ ì‚°ì¶œ ==========
def rerank_with_penalty(user_like_vec, user_dislike_vecs, category_name,
                        top_k=30, alpha=1.0, beta=0.5, dislike_threshold=0.75):
    print(f"ğŸ”  {category_name} í›„ë³´ ê²€ìƒ‰ ì¤‘ (Weaviate near_vector)...")
    results = collection.query.near_vector(
        near_vector=user_like_vec.tolist(),
        limit=top_k * 4,
        return_metadata=["distance"],
        include_vector=True,
        filters=wq.Filter.by_property("category").equal(category_name),
        return_properties=["place_id", "name", "dislike_embedding"]
    )

    scored = []
    for obj in results.objects:
        like_sim = 1 - obj.metadata.distance
        place_dislike_vec = obj.properties.get("dislike_embedding", [])
        max_dislike_sim = 0
        if place_dislike_vec:
            sims = [
                cosine_similarity([ud], [place_dislike_vec])[0][0]
                for ud in user_dislike_vecs if len(ud) > 0
            ]
            max_dislike_sim = max(sims) if sims else 0

        if max_dislike_sim > dislike_threshold:
            continue

        sim_score = alpha * like_sim - beta * max_dislike_sim
        scored.append({
            "id": obj.properties.get("place_id"),
            "name": obj.properties.get("name"),
            "like_sim": float(like_sim),
            "dislike_sim": float(max_dislike_sim),
            "sim_score": float(sim_score)
        })

    print(f"âœ…  {category_name} í›„ë³´ {len(scored)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ\n")
    scored.sort(key=lambda x: x["sim_score"], reverse=True)
    return scored[:top_k]


# ========== 4. GPT ì •ë ¬ ==========
def gpt_sort_with_reviews(user_like, user_dislike, category_name, candidates, review_dict):
    print(f"ğŸ¤–  GPT ìˆœìœ„ ì¬ì •ë ¬ ìš”ì²­ ì¤‘... ({category_name})")

    data_for_gpt = []
    for c in candidates:
        rid = c["id"]
        rc = review_dict.get(rid, 0)

        # âœ… NaN ì•ˆì „ ì²˜ë¦¬
        if pd.isna(rc):
            rc = 0

        data_for_gpt.append({
            "id": rid,
            "name": c["name"],
            "like_sim": round(c["like_sim"], 4),
            "dislike_sim": round(c["dislike_sim"], 4),
            "review_count": int(rc)
        })

    prompt = f"""
You are a ranking assistant for travel places.

User preferences:
- Likes: {user_like}
- Dislikes: {user_dislike}

Each place below has:
- a like similarity score (higher = better match)
- a dislike similarity score (higher = worse match)
- a review count (higher = more popular)

Re-rank these places **only by order**, without changing or generating new fields.
Sort from most to least suitable for the user.

Return ONLY a JSON array of IDs, like this:
[3013409, 10062156, 10045242, 3003412]

Here are the candidates:
{json.dumps(data_for_gpt, ensure_ascii=False, indent=2)}
"""

    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",  # âœ… ë¹ ë¥´ê³  ì €ë ´í•œ ëª¨ë¸
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2
    )

    content = response.choices[0].message.content.strip()
    json_match = re.search(r"\[.*\]", content, re.DOTALL)

    if json_match:
        try:
            sorted_ids = json.loads(json_match.group(0))
        except Exception:
            sorted_ids = []
    else:
        sorted_ids = []

    if not sorted_ids:
        print("âš ï¸ GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨ â€” ì›ë¬¸ ì¶œë ¥ â†“")
        print(content[:500])
    else:
        print(f"âœ… GPT ì •ë ¬ ì™„ë£Œ ({len(sorted_ids)}ê°œ)\n")

    return sorted_ids


# ========== 5. ìœ ì €ë³„ ì „ì²´ ì²˜ë¦¬ ==========
user_df = pd.read_csv(CONFIG["USER_FILE"])
os.makedirs(CONFIG["OUTPUT_DIR"], exist_ok=True)

for idx, user in enumerate(user_df.itertuples(), start=1):
    user_id = user.user_id
    like_keywords = eval(user.like_keywords)
    dislike_keywords = eval(user.dislike_keywords)

    print("=" * 60)
    print(f"ğŸ‘¤ Processing User {idx}/{len(user_df)} â†’ {user_id}")
    print(f"   ğŸ‘ like: {like_keywords}")
    print(f"   ğŸ‘ dislike: {dislike_keywords}")
    print("-" * 60)

    user_like_vec = model.encode(" ".join(like_keywords), convert_to_numpy=True)
    user_dislike_vecs = [model.encode(kw, convert_to_numpy=True) for kw in dislike_keywords]

    results_by_cat = {}

    for cat in CATEGORY_FILES.keys():
        candidates = rerank_with_penalty(user_like_vec, user_dislike_vecs, cat, top_k=CONFIG["TOP_K"])

        df = pd.read_csv(os.path.join(CONFIG["DATA_DIR"], CATEGORY_FILES[cat]))
        review_col = "review_count" if cat == "Accommodation" else "all_review_count"
        review_dict = dict(zip(df["id"], df[review_col]))

        sorted_ids = gpt_sort_with_reviews(
            like_keywords, dislike_keywords,
            CATEGORY_TRANSLATE[cat], candidates, review_dict
        )

        results_by_cat[CATEGORY_TRANSLATE[cat]] = [
            {"id": pid, "category": CATEGORY_TRANSLATE[cat]} for pid in sorted_ids
        ]

    out_path = os.path.join(CONFIG["OUTPUT_DIR"], f"{user_id}_recommendations.json")
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(results_by_cat, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ {user_id} ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {out_path}\n")

client.close()
print("\nğŸ”’ ì „ì²´ ìœ ì € ì²˜ë¦¬ ì™„ë£Œ & ì—°ê²° ì¢…ë£Œ")
