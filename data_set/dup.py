import os
import pandas as pd

BASE_PATH = r"C:\Users\changjin\workspace\lab\pln\data_set\null_X"

def check_id_duplicates(base_path):
    print("=" * 80)
    print("ðŸ” CSV íŒŒì¼ë³„ id ì¤‘ë³µ ìƒì„¸ ê²€ì‚¬ ì‹œìž‘")
    print("=" * 80)

    files = [f for f in os.listdir(base_path) if f.endswith(".csv")]
    if not files:
        print("âš ï¸ CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    for file in files:
        file_path = os.path.join(base_path, file)
        print(f"\nðŸ“‚ {file}")

        try:
            df = pd.read_csv(file_path)
        except Exception as e:
            print(f" âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {e}")
            continue

        if 'id' not in df.columns:
            print(" âš ï¸ 'id' ì»¬ëŸ¼ ì—†ìŒ â€” ê±´ë„ˆëœ€")
            continue

        total_count = len(df)
        unique_count = df['id'].nunique()
        dup_count = total_count - unique_count

        print(f" - ì´ í–‰ ìˆ˜: {total_count}")
        print(f" - ê³ ìœ  id ìˆ˜: {unique_count}")

        if dup_count > 0:
            dup_df = df[df['id'].duplicated(keep=False)].copy()
            dup_df["duplicate_group_count"] = dup_df.groupby("id")["id"].transform("count")
            print(f" âŒ ì¤‘ë³µ ID {dup_count}ê°œ ë°œê²¬ â€” ìƒì„¸ ëª©ë¡:")
            for dup_id, group in dup_df.groupby("id"):
                idx_list = group.index.tolist()
                count = len(idx_list)
                print(f"    â–¶ ID {dup_id} ({count}íšŒ) â†’ í–‰ ìœ„ì¹˜: {idx_list}")
        else:
            print(" âœ… ì¤‘ë³µ ì—†ìŒ")

        if total_count > 0:
            print(f" - id íƒ€ìž… ì˜ˆì‹œ: {type(df['id'].iloc[0])}")

    print("\nâœ… ê²€ì‚¬ ì™„ë£Œ")
    print("=" * 80)


if __name__ == "__main__":
    check_id_duplicates(BASE_PATH)
