import os
import pandas as pd
import numpy as np
from haversine import haversine, Unit
import matplotlib.pyplot as plt
import json
from matplotlib import font_manager, rc

# ========================================
# ğŸ”¤ í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
# ========================================
font_path = "C:/Windows/Fonts/malgun.ttf"
font_name = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_name)
plt.rcParams['axes.unicode_minus'] = False


# ========================================
# ğŸ“ ê±°ë¦¬ ê³„ì‚°
# ========================================
def haversine_km(coord1, coord2):
    return haversine(coord1, coord2, unit=Unit.KILOMETERS)


# ========================================
# ğŸ§© 1ë‹¨ê³„: í¬ê·€ ì¹´í…Œê³ ë¦¬ ìš°ì„  Greedy Cluster
# ========================================
def greedy_cluster(df, target_cluster_size=20, base_radius=0.8, min_per_cat=3):
    """
    ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ìœ¼ë¡œ ê· í˜•ì¡íŒ í´ëŸ¬ìŠ¤í„° ìƒì„±
    
    Args:
        target_cluster_size: ëª©í‘œ í´ëŸ¬ìŠ¤í„° í¬ê¸° (ê¸°ë³¸ 50ê°œ)
        min_per_cat: ê° ì¹´í…Œê³ ë¦¬ ìµœì†Œ ê°œìˆ˜ (ê¸°ë³¸ 3ê°œ)
    """
    df = df.copy()
    df["assigned"] = False
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    clusters = []
    cluster_id = 0
    
    # ğŸ”‘ ì¹´í…Œê³ ë¦¬ë³„ ë¹„ìœ¨ ê³„ì‚°
    category_counts = df["category"].value_counts().to_dict()
    total_places = len(df)
    category_ratios = {cat: category_counts.get(cat, 0) / total_places for cat in all_categories}
    
    # í¬ê·€í•œ ìˆœì„œë¡œ ì •ë ¬ (ê°œìˆ˜ ì ì€ ìˆœ)
    rarity_order = sorted(category_counts.keys(), key=lambda x: category_counts[x])
    
    print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë° ë¹„ìœ¨:")
    for cat in all_categories:
        count = category_counts.get(cat, 0)
        ratio = category_ratios[cat]
        print(f"  {cat}: {count}ê°œ ({ratio*100:.1f}%)")
    
    # ğŸ”‘ ê° í´ëŸ¬ìŠ¤í„°ì˜ ëª©í‘œ ê°œìˆ˜ ê³„ì‚° (ë¹„ìœ¨ ê¸°ë°˜)
    target_per_cat = {}
    for cat in all_categories:
        calculated = int(target_cluster_size * category_ratios[cat])
        # ìµœì†Œ ê°œìˆ˜ ë³´ì¥
        target_per_cat[cat] = max(min_per_cat, calculated)
    
    print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„°ë‹¹ ëª©í‘œ ê°œìˆ˜ (ì´ {sum(target_per_cat.values())}ê°œ):")
    for cat in all_categories:
        print(f"  {cat}: {target_per_cat[cat]}ê°œ")

    while not df[df["assigned"] == False].empty:
        # í¬ê·€í•œ ì¹´í…Œê³ ë¦¬ë¶€í„° seedë¡œ ì„ íƒ
        seed_idx = None
        for cat in rarity_order:
            available = df[(df["assigned"] == False) & (df["category"] == cat)]
            if len(available) > 0:
                seed_idx = available.index[0]
                break
        
        # í¬ê·€ ì¹´í…Œê³ ë¦¬ê°€ ëª¨ë‘ ì†Œì§„ë˜ë©´ ì¼ë°˜ ìˆœì„œë¡œ
        if seed_idx is None:
            seed_idx = df[df["assigned"] == False].index[0]
        
        seed = df.loc[seed_idx]
        cluster_points = [seed_idx]
        df.at[seed_idx, "assigned"] = True

        # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì´ˆê¸°í™”
        cat_count = {cat: 0 for cat in all_categories}
        cat_count[seed["category"]] += 1

        df["distance"] = df.apply(
            lambda r: haversine_km(
                (seed["latitude"], seed["longitude"]),
                (r["latitude"], r["longitude"])
            ), axis=1
        )

        sorted_idx = df[df["assigned"] == False].sort_values("distance").index

        for i in sorted_idx:
            # ğŸ”‘ ëª¨ë“  ì¹´í…Œê³ ë¦¬ê°€ ëª©í‘œ ê°œìˆ˜ ë„ë‹¬í•˜ë©´ ì¢…ë£Œ
            if all(cat_count[cat] >= target_per_cat[cat] for cat in all_categories):
                break
                
            point = df.loc[i]
            dist = point["distance"]
            radius = base_radius * (1 + len(cluster_points) / 20)

            if dist > radius:
                continue

            cat = point["category"]
            
            # ğŸ”‘ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ê°€ ëª©í‘œ ê°œìˆ˜ ë„ë‹¬í•˜ë©´ ìŠ¤í‚µ
            if cat_count[cat] >= target_per_cat[cat]:
                continue

            cluster_points.append(i)
            df.at[i, "assigned"] = True
            cat_count[cat] += 1

        df.loc[cluster_points, "cluster_id"] = cluster_id
        
        # ë””ë²„ê·¸ ì¶œë ¥
        total = sum(cat_count.values())
        diversity = sum(1 for count in cat_count.values() if count > 0)
        print(f"  C{cluster_id}: ì´ {total}ê°œ (ë‹¤ì–‘ì„±: {diversity}/4) | " + 
              " | ".join([f"{cat[:3]}: {cat_count[cat]}" for cat in all_categories]))
        
        cluster_id += 1
        clusters.append(cluster_points)

    # ë¯¸í• ë‹¹ ì¥ì†Œ ì²˜ë¦¬ - ê°€ê¹Œìš´ ê²ƒë¼ë¦¬ ì†Œí˜• í´ëŸ¬ìŠ¤í„° ìƒì„±
    unassigned = df[df["assigned"] == False]
    if len(unassigned) > 0:
        print(f"\nâš ï¸ ë¯¸í• ë‹¹ ì¥ì†Œ {len(unassigned)}ê°œ â†’ ì†Œí˜• í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™”")
        
        unassigned_df = df[df["assigned"] == False].copy()
        
        while len(unassigned_df) > 0:
            # í¬ê·€ ì¹´í…Œê³ ë¦¬ë¶€í„° ì‹œë“œë¡œ
            seed_idx = None
            for cat in rarity_order:
                available = unassigned_df[unassigned_df["category"] == cat]
                if len(available) > 0:
                    seed_idx = available.index[0]
                    break
            
            if seed_idx is None:
                seed_idx = unassigned_df.index[0]
            
            seed = unassigned_df.loc[seed_idx]
            
            small_cluster = [seed_idx]
            df.at[seed_idx, "assigned"] = True
            unassigned_df = unassigned_df.drop(seed_idx)
            
            # 2km ë°˜ê²½ ë‚´ì˜ ë¯¸í• ë‹¹ ì¥ì†Œë“¤ì„ ë¬¶ê¸°
            for idx, row in unassigned_df.iterrows():
                dist = haversine_km(
                    (seed["latitude"], seed["longitude"]),
                    (row["latitude"], row["longitude"])
                )
                if dist <= 2.0:  # 2km ì´ë‚´
                    small_cluster.append(idx)
                    df.at[idx, "assigned"] = True
            
            # í´ëŸ¬ìŠ¤í„° í• ë‹¹
            df.loc[small_cluster, "cluster_id"] = cluster_id
            
            cat_count = df.loc[small_cluster, "category"].value_counts().to_dict()
            total = len(small_cluster)
            diversity = sum(1 for cat in all_categories if cat_count.get(cat, 0) > 0)
            print(f"  C{cluster_id}: ì´ {total}ê°œ (ì†Œí˜•, ë‹¤ì–‘ì„±: {diversity}/4) | " + 
                  " | ".join([f"{cat[:3]}: {cat_count.get(cat, 0)}" 
                             for cat in all_categories]))
            
            cluster_id += 1
            
            # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•´ ì—…ë°ì´íŠ¸
            unassigned_df = df[df["assigned"] == False].copy()

    return df


# ========================================
# ğŸ”„ 2ë‹¨ê³„: ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„° ë³‘í•©
# ========================================
def merge_unsatisfied_clusters(df, min_per_cat=3, ideal_per_cat=10, max_cluster_size=50, merge_radius=5.0, max_iterations=10):
    """
    ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ë§Œ ë³‘í•©
    
    Args:
        min_per_cat: ìµœì†Œ í—ˆìš© ê°œìˆ˜ (ê¸°ë³¸ 3ê°œ)
        ideal_per_cat: ì´ìƒì ì¸ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
        max_cluster_size: ë³‘í•© í›„ ìµœëŒ€ í¬ê¸°
    """
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    
    for iteration in range(max_iterations):
        print(f"\nğŸ”„ ë³‘í•© ë°˜ë³µ {iteration + 1}íšŒì°¨...")
        
        # ë§¤ ë°˜ë³µë§ˆë‹¤ í˜„ì¬ ìƒíƒœ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜
        acceptable_clusters = set()  # í—ˆìš© ê°€ëŠ¥ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ 3ê°œ ì´ìƒ)
        critical_clusters = []       # ê¸´ê¸‰ ë³‘í•© í•„ìš” (3ê°œ ë¯¸ë§Œ ì¹´í…Œê³ ë¦¬ ìˆìŒ)
        
        for cid, group in df.groupby("cluster_id"):
            cat_counts = group["category"].value_counts().to_dict()
            counts = {cat: cat_counts.get(cat, 0) for cat in all_categories}
            total_size = len(group)
            
            # ìµœì†Œ í—ˆìš© ì¡°ê±´: ëª¨ë“  ì¹´í…Œê³ ë¦¬ê°€ ìµœì†Œ 3ê°œ ì´ìƒ
            has_insufficient = any(count < min_per_cat for count in counts.values())
            
            if not has_insufficient:
                acceptable_clusters.add(cid)  # 3ê°œ ì´ìƒ â†’ í—ˆìš©
            else:
                # ë¶€ì¡±í•œ ì •ë„ë¡œ ìš°ì„ ìˆœìœ„ ê²°ì •
                insufficient_score = sum(max(0, min_per_cat - count) * 10 for count in counts.values())
                zero_count = sum(1 for count in counts.values() if count == 0)
                diversity = sum(1 for count in counts.values() if count > 0)
                priority = zero_count * 1000 + insufficient_score + (4 - diversity) * 10
                critical_clusters.append((priority, cid, total_size))
        
        print(f"  âœ… í—ˆìš© ê°€ëŠ¥ í´ëŸ¬ìŠ¤í„°: {len(acceptable_clusters)}ê°œ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ 3ê°œ ì´ìƒ)")
        print(f"  ğŸ”´ ê¸´ê¸‰ ë³‘í•© í•„ìš”: {len(critical_clusters)}ê°œ (3ê°œ ë¯¸ë§Œ ì¹´í…Œê³ ë¦¬ ìˆìŒ)")
        
        if not critical_clusters:
            print(f"âœ… ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ ìµœì†Œ ì¡°ê±´ ë§Œì¡±! (ë°˜ë³µ {iteration + 1}íšŒ)")
            break
        
        # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë¶€ì¡±í•œ ê²ƒ ë¨¼ì €)
        critical_clusters.sort(reverse=True)
        merged_count = 0
        
        for priority, cid, size1 in critical_clusters:
            if cid not in df["cluster_id"].values:
                continue
            
            # ì‹¤ì‹œê°„ ì¬í™•ì¸
            current_group = df[df["cluster_id"] == cid]
            current_counts = current_group["category"].value_counts().to_dict()
            current_dict = {cat: current_counts.get(cat, 0) for cat in all_categories}
            
            # ì´ë¯¸ ëª¨ë“  ì¹´í…Œê³ ë¦¬ê°€ 3ê°œ ì´ìƒì´ë©´ ë” ì´ìƒ ë³‘í•© ë¶ˆí•„ìš”
            if all(count >= min_per_cat for count in current_dict.values()):
                continue
            
            center1 = current_group[["latitude", "longitude"]].mean()
            best_target = None
            best_score = -1
            
            # ë³‘í•© ëŒ€ìƒ ì°¾ê¸° (ê¸´ê¸‰ í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ë§Œ)
            for _, target_cid, size2 in critical_clusters:
                if target_cid == cid or target_cid not in df["cluster_id"].values:
                    continue
                
                # íƒ€ê²Ÿë„ ì‹¤ì‹œê°„ ì¬í™•ì¸
                target_group = df[df["cluster_id"] == target_cid]
                target_counts = target_group["category"].value_counts().to_dict()
                target_dict = {cat: target_counts.get(cat, 0) for cat in all_categories}
                
                # íƒ€ê²Ÿì´ ì´ë¯¸ í—ˆìš© ê°€ëŠ¥í•´ì¡Œìœ¼ë©´ ì œì™¸
                if all(count >= min_per_cat for count in target_dict.values()):
                    continue
                
                # ë³‘í•© í›„ í¬ê¸° ì œí•œ
                merged_size = len(current_group) + len(target_group)
                if merged_size > max_cluster_size:
                    continue
                
                center2 = target_group[["latitude", "longitude"]].mean()
                dist = haversine_km(tuple(center1), tuple(center2))
                
                if dist > merge_radius:
                    continue
                
                # ìƒí˜¸ ë³´ì™„ ì ìˆ˜ ê³„ì‚°
                complementary_score = 0
                for cat in all_categories:
                    c1 = current_dict[cat]
                    c2 = target_dict[cat]
                    
                    # 0ê°œ â†’ 1ê°œ ì´ìƒ: ì´ˆê³  ì ìˆ˜
                    if c1 == 0 and c2 > 0:
                        complementary_score += c2 * 50
                    elif c2 == 0 and c1 > 0:
                        complementary_score += c1 * 50
                    # 1~2ê°œ â†’ 3ê°œ ì´ìƒ: ê³  ì ìˆ˜
                    elif c1 < min_per_cat and c2 > 0:
                        needed = min_per_cat - c1
                        complementary_score += min(c2, needed) * 20
                    elif c2 < min_per_cat and c1 > 0:
                        needed = min_per_cat - c2
                        complementary_score += min(c1, needed) * 20
                    # ë‘˜ ë‹¤ ìˆì§€ë§Œ ë¶€ì¡±: ì¼ë°˜ ì ìˆ˜
                    elif c1 < ideal_per_cat and c2 > 0:
                        complementary_score += c2
                    elif c2 < ideal_per_cat and c1 > 0:
                        complementary_score += c1
                
                # ì ìˆ˜: ìƒí˜¸ë³´ì™„ + ê±°ë¦¬ í˜ë„í‹°
                score = complementary_score - dist * 2
                
                if score > best_score:
                    best_score = score
                    best_target = target_cid
            
            # ë³‘í•© ì‹¤í–‰
            if best_target is not None:
                df.loc[df["cluster_id"] == cid, "cluster_id"] = best_target
                merged_count += 1
                dist_to_target = haversine_km(
                    tuple(center1),
                    tuple(df[df["cluster_id"] == best_target][["latitude", "longitude"]].mean())
                )
                merged_size = len(df[df["cluster_id"] == best_target])
                print(f"  âš¡ C{cid} â†’ C{best_target} ë³‘í•© (ê±°ë¦¬: {dist_to_target:.2f}km, ë³‘í•© í›„: {merged_size}ê°œ)")
        
        if merged_count == 0:
            print(f"âš ï¸ ë” ì´ìƒ ë³‘í•©í•  ìˆ˜ ì—†ìŒ (ë°˜ë³µ {iteration + 1}íšŒ)")
            print(f"  ğŸ’¡ merge_radiusë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ max_cluster_sizeë¥¼ ëŠ˜ë ¤ë³´ì„¸ìš”")
            break
        
        # í´ëŸ¬ìŠ¤í„° ID ì¬ì •ë ¬
        df["cluster_id"] = df["cluster_id"].astype(int)
        new_ids = {old: new for new, old in enumerate(sorted(df["cluster_id"].unique()))}
        df["cluster_id"] = df["cluster_id"].map(new_ids)
        
        print(f"  ğŸ“Š í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(df['cluster_id'].unique())}")
    
    return df


# ========================================
# ğŸ§± Hotzone JSON ìƒì„±
# ========================================
def build_hotzones(df):
    hotzones = []
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]

    for cid, group in df.groupby("cluster_id"):
        center_lat = group["latitude"].mean()
        center_lng = group["longitude"].mean()

        categories = {}
        for cat in all_categories:
            if cat in group["category"].unique():
                cat_group = group[group["category"] == cat]
                items = [
                    {
                        "id": int(row["id"]) if "id" in row and not pd.isna(row["id"]) else idx,
                        "name": row["name"],
                        "final_score": round(np.random.uniform(0.5, 0.9), 2)
                    }
                    for idx, row in cat_group.iterrows()
                ]
                categories[cat] = items
            else:
                categories[cat] = []

        hotzone = {
            "cluster_id": int(cid),
            "center_lat": round(center_lat, 6),
            "center_lng": round(center_lng, 6),
            "hotzone_score": round(
                min(1.0, len(group) / 50 + len([v for v in categories.values() if v]) * 0.1), 2
            ),
            "category_diversity": sum(len(v) > 0 for v in categories.values()),
            "total_places": len(group),
            "categories": categories
        }
        hotzones.append(hotzone)
    return {"hotzones": hotzones}


# ========================================
# ğŸ¨ ì‹œê°í™”
# ========================================
def visualize_clusters(df, save_path, title):
    plt.figure(figsize=(10, 8))
    clusters = sorted(df["cluster_id"].unique(), key=int)
    colors = plt.cm.tab20(np.linspace(0, 1, len(clusters)))

    for color, cid in zip(colors, clusters):
        sub = df[df["cluster_id"] == cid]
        plt.scatter(sub["longitude"], sub["latitude"], s=25, color=color, alpha=0.8, label=f"C{cid}")

    plt.xlabel("Longitude (ê²½ë„)")
    plt.ylabel("Latitude (ìœ„ë„)")
    plt.title(title)
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=7)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=200)
    print(f"âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close()


# ========================================
# ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë³´ê³ ì„œ
# ========================================
def print_cluster_report(df, min_acceptable=3, ideal_per_cat=10, max_size=50):
    """
    í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë³´ê³ ì„œ
    """
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    print("\n" + "="*70)
    print("ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë³´ê³ ì„œ")
    print("="*70)
    
    acceptable_count = 0      # ëª¨ë“  ì¹´í…Œê³ ë¦¬ 3ê°œ ì´ìƒ
    ideal_count = 0           # ëª¨ë“  ì¹´í…Œê³ ë¦¬ 10ê°œ ì´ìƒ
    critical_count = 0        # 3ê°œ ë¯¸ë§Œ ì¹´í…Œê³ ë¦¬ ìˆìŒ
    oversized_count = 0
    
    for cid, group in df.groupby("cluster_id"):
        cat_counts = group["category"].value_counts().to_dict()
        counts = {cat: cat_counts.get(cat, 0) for cat in all_categories}
        total = len(group)
        diversity = sum(1 for count in counts.values() if count > 0)
        
        has_insufficient = any(count < min_acceptable for count in counts.values())
        is_ideal = all(count >= ideal_per_cat for count in counts.values())
        is_oversized = total > max_size
        
        # ìƒíƒœ ê²°ì •
        if has_insufficient:
            status = "ğŸ”´ ê¸´ê¸‰"
            critical_count += 1
        elif is_ideal:
            status = "âœ… ì´ìƒì "
            ideal_count += 1
            acceptable_count += 1
        else:
            status = "âš ï¸ í—ˆìš©"
            acceptable_count += 1
        
        if is_oversized:
            status += " ğŸ”´ ì´ˆê³¼"
            oversized_count += 1
        
        print(f"\nğŸ·ï¸ í´ëŸ¬ìŠ¤í„° {cid} [{status}] (ì´ {total}ê°œ)")
        print(f"  ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„±: {diversity}/4")
        for cat in all_categories:
            count = counts[cat]
            if count < min_acceptable:
                mark = "ğŸ”´"  # 3ê°œ ë¯¸ë§Œì€ ê¸´ê¸‰
            elif count >= ideal_per_cat:
                mark = "âœ“"  # 10ê°œ ì´ìƒì€ ì´ìƒì 
            else:
                mark = "âš "  # 3~9ê°œëŠ” í—ˆìš©
            print(f"  {mark} {cat}: {count}ê°œ")
        
        if is_oversized:
            print(f"  ğŸ”´ í¬ê¸° ì´ˆê³¼: {max_size}ê°œ ì œí•œì„ {total - max_size}ê°œ ì´ˆê³¼!")
    
    print("\n" + "="*70)
    print(f"âœ… ì´ìƒì  í´ëŸ¬ìŠ¤í„°: {ideal_count}ê°œ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ 10ê°œ ì´ìƒ)")
    print(f"âš ï¸ í—ˆìš© í´ëŸ¬ìŠ¤í„°: {acceptable_count - ideal_count}ê°œ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ 3ê°œ ì´ìƒ)")
    print(f"ğŸ”´ ê¸´ê¸‰ ë³‘í•© í•„ìš”: {critical_count}ê°œ (3ê°œ ë¯¸ë§Œ ì¹´í…Œê³ ë¦¬ ìˆìŒ)")
    if oversized_count > 0:
        print(f"ğŸ”´ í¬ê¸° ì´ˆê³¼ í´ëŸ¬ìŠ¤í„°: {oversized_count}ê°œ")
    print("="*70)


# ========================================
# ğŸš€ ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    base_dir = r"C:\Users\changjin\workspace\lab\pln"
    data_path = os.path.join(base_dir, "data_set", "clustering_category_combine.csv")
    out_dir = os.path.join(base_dir, "clustering")
    os.makedirs(out_dir, exist_ok=True)

    df = pd.read_csv(data_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)}ê°œ ì¥ì†Œ)")

    # 1ï¸âƒ£ ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ Greedy í´ëŸ¬ìŠ¤í„°ë§
    print("\n" + "="*70)
    print("ğŸ“ 1ë‹¨ê³„: ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ Greedy í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
    print("="*70)
    clustered = greedy_cluster(
        df, 
        target_cluster_size=50,  # ëª©í‘œ í´ëŸ¬ìŠ¤í„° í¬ê¸° 50ê°œ (ë¹„ìœ¨ì— ë”°ë¼ ë¶„ë°°)
        base_radius=0.8,         # ê¸°ë³¸ ë°˜ê²½ 0.8km
        min_per_cat=3            # ê° ì¹´í…Œê³ ë¦¬ ìµœì†Œ 3ê°œ ë³´ì¥
    )
    print(f"\nğŸ“Š 1ì°¨ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(clustered['cluster_id'].unique())}")

    # 2ï¸âƒ£ ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ë§Œ ë³‘í•© (3ê°œ ë¯¸ë§Œ ì¹´í…Œê³ ë¦¬ í•´ì†Œ)
    merged = merge_unsatisfied_clusters(
        clustered, 
        min_per_cat=3,         # ìµœì†Œ 3ê°œ ì´ìƒ (3ê°œ ì´ìƒì´ë©´ OK) â­
        ideal_per_cat=10,      # ì´ìƒì ìœ¼ë¡œëŠ” 10ê°œ (ì°¸ê³ ìš©)
        max_cluster_size=70,   # ë³‘í•© í›„ ìµœëŒ€ 70ê°œ ì œí•œ (ë¹„ìœ¨ ê¸°ë°˜ì´ë¼ ì¢€ ë” ì—¬ìœ ìˆê²Œ)
        merge_radius=5.0,      # ë³‘í•© ë°˜ê²½ 5km
        max_iterations=10      # ìµœëŒ€ 10íšŒ ë°˜ë³µ
    )
    print(f"ğŸ“Š ë³‘í•© í›„ ìµœì¢… í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(merged['cluster_id'].unique())}")

    # 3ï¸âƒ£ í’ˆì§ˆ ë³´ê³ ì„œ ì¶œë ¥
    print_cluster_report(merged, min_acceptable=3, ideal_per_cat=10, max_size=70)

    # 4ï¸âƒ£ ì‹œê°í™”
    visualize_clusters(clustered, os.path.join(out_dir, "step1_greedy.png"), "ğŸ“ 1ë‹¨ê³„ Greedy í´ëŸ¬ìŠ¤í„°ë§")
    visualize_clusters(merged, os.path.join(out_dir, "step2_merged.png"), "ğŸ“ 2ë‹¨ê³„ ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„° ë³‘í•©")

    # 5ï¸âƒ£ JSON ì €ì¥
    hotzones = build_hotzones(merged)
    json_path = os.path.join(out_dir, "greedy_hotzones_merged.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(hotzones, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Hotzones JSON ì €ì¥ ì™„ë£Œ: {json_path}")

    print("\nâœ¨ ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ + 3ê°œ ìµœì†Œ ê¸°ì¤€ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ!")