import os
import pandas as pd
import numpy as np
from haversine import haversine, Unit
import matplotlib.pyplot as plt
import json
from matplotlib import font_manager, rc

# ========================================
# ğŸ”¤ í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
# ========================================
font_path = "C:/Windows/Fonts/malgun.ttf"
font_name = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_name)
plt.rcParams['axes.unicode_minus'] = False


# ========================================
# ğŸ“ ê±°ë¦¬ ê³„ì‚°
# ========================================
def haversine_km(coord1, coord2):
    return haversine(coord1, coord2, unit=Unit.KILOMETERS)


# ========================================
# ğŸ§© 1ë‹¨ê³„: ë¹„ìœ¨ ê¸°ë°˜ Greedy Cluster
# ========================================
def greedy_cluster(df, target_cluster_size=50, base_radius=0.8, min_per_cat=3):
    """
    ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ìœ¼ë¡œ ê· í˜•ì¡íŒ í´ëŸ¬ìŠ¤í„° ìƒì„±
    """
    df = df.copy()
    df["assigned"] = False
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    clusters = []
    cluster_id = 0
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¹„ìœ¨ ê³„ì‚°
    category_counts = df["category"].value_counts().to_dict()
    total_places = len(df)
    category_ratios = {cat: category_counts.get(cat, 0) / total_places for cat in all_categories}
    
    # í¬ê·€í•œ ìˆœì„œë¡œ ì •ë ¬
    rarity_order = sorted(category_counts.keys(), key=lambda x: category_counts[x])
    
    print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„í¬ ë° ë¹„ìœ¨:")
    for cat in all_categories:
        count = category_counts.get(cat, 0)
        ratio = category_ratios[cat]
        print(f"  {cat}: {count}ê°œ ({ratio*100:.1f}%)")
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ëª©í‘œ ê°œìˆ˜ ê³„ì‚°
    target_per_cat = {}
    for cat in all_categories:
        calculated = int(target_cluster_size * category_ratios[cat])
        target_per_cat[cat] = max(min_per_cat, calculated)
    
    print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„°ë‹¹ ëª©í‘œ ê°œìˆ˜ (ì´ {sum(target_per_cat.values())}ê°œ):")
    for cat in all_categories:
        print(f"  {cat}: {target_per_cat[cat]}ê°œ")

    while not df[df["assigned"] == False].empty:
        # í¬ê·€í•œ ì¹´í…Œê³ ë¦¬ë¶€í„° seedë¡œ ì„ íƒ
        seed_idx = None
        for cat in rarity_order:
            available = df[(df["assigned"] == False) & (df["category"] == cat)]
            if len(available) > 0:
                seed_idx = available.index[0]
                break
        
        if seed_idx is None:
            seed_idx = df[df["assigned"] == False].index[0]
        
        seed = df.loc[seed_idx]
        cluster_points = [seed_idx]
        df.at[seed_idx, "assigned"] = True

        cat_count = {cat: 0 for cat in all_categories}
        cat_count[seed["category"]] += 1

        df["distance"] = df.apply(
            lambda r: haversine_km(
                (seed["latitude"], seed["longitude"]),
                (r["latitude"], r["longitude"])
            ), axis=1
        )

        sorted_idx = df[df["assigned"] == False].sort_values("distance").index

        for i in sorted_idx:
            if all(cat_count[cat] >= target_per_cat[cat] for cat in all_categories):
                break
                
            point = df.loc[i]
            dist = point["distance"]
            radius = base_radius * (1 + len(cluster_points) / 20)

            if dist > radius:
                continue

            cat = point["category"]
            
            if cat_count[cat] >= target_per_cat[cat]:
                continue

            cluster_points.append(i)
            df.at[i, "assigned"] = True
            cat_count[cat] += 1

        df.loc[cluster_points, "cluster_id"] = cluster_id
        
        total = sum(cat_count.values())
        diversity = sum(1 for count in cat_count.values() if count > 0)
        print(f"  C{cluster_id}: ì´ {total}ê°œ (ë‹¤ì–‘ì„±: {diversity}/4) | " + 
              " | ".join([f"{cat[:3]}: {cat_count[cat]}" for cat in all_categories]))
        
        cluster_id += 1
        clusters.append(cluster_points)

    # ë¯¸í• ë‹¹ ì¥ì†Œ ì²˜ë¦¬
    unassigned = df[df["assigned"] == False]
    if len(unassigned) > 0:
        print(f"\nâš ï¸ ë¯¸í• ë‹¹ ì¥ì†Œ {len(unassigned)}ê°œ â†’ ì†Œí˜• í´ëŸ¬ìŠ¤í„°ë¡œ ê·¸ë£¹í™”")
        
        unassigned_df = df[df["assigned"] == False].copy()
        
        while len(unassigned_df) > 0:
            seed_idx = None
            for cat in rarity_order:
                available = unassigned_df[unassigned_df["category"] == cat]
                if len(available) > 0:
                    seed_idx = available.index[0]
                    break
            
            if seed_idx is None:
                seed_idx = unassigned_df.index[0]
            
            seed = unassigned_df.loc[seed_idx]
            
            small_cluster = [seed_idx]
            df.at[seed_idx, "assigned"] = True
            unassigned_df = unassigned_df.drop(seed_idx)
            
            for idx, row in unassigned_df.iterrows():
                dist = haversine_km(
                    (seed["latitude"], seed["longitude"]),
                    (row["latitude"], row["longitude"])
                )
                if dist <= 2.0:
                    small_cluster.append(idx)
                    df.at[idx, "assigned"] = True
            
            df.loc[small_cluster, "cluster_id"] = cluster_id
            
            cat_count = df.loc[small_cluster, "category"].value_counts().to_dict()
            total = len(small_cluster)
            diversity = sum(1 for cat in all_categories if cat_count.get(cat, 0) > 0)
            print(f"  C{cluster_id}: ì´ {total}ê°œ (ì†Œí˜•, ë‹¤ì–‘ì„±: {diversity}/4) | " + 
                  " | ".join([f"{cat[:3]}: {cat_count.get(cat, 0)}" 
                             for cat in all_categories]))
            
            cluster_id += 1
            unassigned_df = df[df["assigned"] == False].copy()

    return df


# ========================================
# ğŸ”„ 2ë‹¨ê³„: ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„° ë³‘í•©
# ========================================
def merge_unsatisfied_clusters(df, min_per_cat=3, ideal_per_cat=10, min_cluster_size=15, max_cluster_size=70, merge_radius=8.0, max_iterations=15):
    """
    ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ë§Œ ë³‘í•©
    """
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    
    for iteration in range(max_iterations):
        print(f"\nğŸ”„ ë³‘í•© ë°˜ë³µ {iteration + 1}íšŒì°¨...")
        
        acceptable_clusters = set()
        critical_clusters = []
        
        for cid, group in df.groupby("cluster_id"):
            cat_counts = group["category"].value_counts().to_dict()
            counts = {cat: cat_counts.get(cat, 0) for cat in all_categories}
            total_size = len(group)
            
            is_too_small = total_size < min_cluster_size
            has_insufficient = any(count < min_per_cat for count in counts.values())
            
            if is_too_small or has_insufficient:
                size_penalty = max(0, min_cluster_size - total_size) * 5
                insufficient_score = sum(max(0, min_per_cat - count) * 10 for count in counts.values())
                zero_count = sum(1 for count in counts.values() if count == 0)
                diversity = sum(1 for count in counts.values() if count > 0)
                priority = zero_count * 1000 + size_penalty + insufficient_score + (4 - diversity) * 10
                critical_clusters.append((priority, cid, total_size))
            else:
                acceptable_clusters.add(cid)
        
        print(f"  âœ… í—ˆìš© ê°€ëŠ¥ í´ëŸ¬ìŠ¤í„°: {len(acceptable_clusters)}ê°œ")
        print(f"  ğŸ”´ ê¸´ê¸‰ ë³‘í•© í•„ìš”: {len(critical_clusters)}ê°œ")
        
        if not critical_clusters:
            print(f"âœ… ëª¨ë“  í´ëŸ¬ìŠ¤í„°ê°€ ìµœì†Œ ì¡°ê±´ ë§Œì¡±! (ë°˜ë³µ {iteration + 1}íšŒ)")
            break
        
        critical_clusters.sort(reverse=True)
        merged_count = 0
        
        for priority, cid, size1 in critical_clusters:
            if cid not in df["cluster_id"].values:
                continue
            
            current_group = df[df["cluster_id"] == cid]
            current_counts = current_group["category"].value_counts().to_dict()
            current_dict = {cat: current_counts.get(cat, 0) for cat in all_categories}
            current_size = len(current_group)
            
            if current_size >= min_cluster_size and all(count >= min_per_cat for count in current_dict.values()):
                continue
            
            center1 = current_group[["latitude", "longitude"]].mean()
            best_target = None
            best_score = -1
            
            for _, target_cid, size2 in critical_clusters:
                if target_cid == cid or target_cid not in df["cluster_id"].values:
                    continue
                
                target_group = df[df["cluster_id"] == target_cid]
                target_counts = target_group["category"].value_counts().to_dict()
                target_dict = {cat: target_counts.get(cat, 0) for cat in all_categories}
                target_size = len(target_group)
                
                if target_size >= min_cluster_size and all(count >= min_per_cat for count in target_dict.values()):
                    continue
                
                merged_size = current_size + target_size
                if merged_size > max_cluster_size:
                    continue
                
                center2 = target_group[["latitude", "longitude"]].mean()
                dist = haversine_km(tuple(center1), tuple(center2))
                
                if dist > merge_radius:
                    continue
                
                complementary_score = 0
                
                size_score = (min_cluster_size - current_size) + (min_cluster_size - target_size)
                complementary_score += max(0, size_score) * 3
                
                for cat in all_categories:
                    c1 = current_dict[cat]
                    c2 = target_dict[cat]
                    
                    if c1 == 0 and c2 > 0:
                        complementary_score += c2 * 50
                    elif c2 == 0 and c1 > 0:
                        complementary_score += c1 * 50
                    elif c1 < min_per_cat and c2 > 0:
                        needed = min_per_cat - c1
                        complementary_score += min(c2, needed) * 20
                    elif c2 < min_per_cat and c1 > 0:
                        needed = min_per_cat - c2
                        complementary_score += min(c1, needed) * 20
                    elif c1 < ideal_per_cat and c2 > 0:
                        complementary_score += c2
                    elif c2 < ideal_per_cat and c1 > 0:
                        complementary_score += c1
                
                score = complementary_score - dist * 1.5
                
                if score > best_score:
                    best_score = score
                    best_target = target_cid
            
            if best_target is not None:
                df.loc[df["cluster_id"] == cid, "cluster_id"] = best_target
                merged_count += 1
                dist_to_target = haversine_km(
                    tuple(center1),
                    tuple(df[df["cluster_id"] == best_target][["latitude", "longitude"]].mean())
                )
                merged_size = len(df[df["cluster_id"] == best_target])
                print(f"  âš¡ C{cid}({current_size}ê°œ) â†’ C{best_target} ë³‘í•© (ê±°ë¦¬: {dist_to_target:.2f}km, ë³‘í•© í›„: {merged_size}ê°œ)")
        
        if merged_count == 0:
            print(f"âš ï¸ ë” ì´ìƒ ë³‘í•©í•  ìˆ˜ ì—†ìŒ (ë°˜ë³µ {iteration + 1}íšŒ)")
            break
        
        df["cluster_id"] = df["cluster_id"].astype(int)
        new_ids = {old: new for new, old in enumerate(sorted(df["cluster_id"].unique()))}
        df["cluster_id"] = df["cluster_id"].map(new_ids)
        
        print(f"  ğŸ“Š í˜„ì¬ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(df['cluster_id'].unique())}")
    
    return df


# ========================================
# ğŸ§± Hotzone JSON ìƒì„±
# ========================================
def build_hotzones(df, target_per_cat=None):
    """
    Hotzone JSON ìƒì„± - ì¹´í…Œê³ ë¦¬ë³„ ê³ ì • ê°œìˆ˜ í• ë‹¹
    """
    hotzones = []
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    
    # ì¹´í…Œê³ ë¦¬ë³„ ëª©í‘œ ê°œìˆ˜ ì„¤ì •
    if target_per_cat is None:
        target_per_cat = {
            "Accommodation": 30,
            "Cafe": 10,
            "Restaurant": 25,
            "Attraction": 10
        }
    
    print(f"\nğŸ¯ ì¹´í…Œê³ ë¦¬ë³„ ëª©í‘œ ê°œìˆ˜:")
    for cat in all_categories:
        print(f"  {cat}: {target_per_cat[cat]}ê°œ")

    for cid, group in df.groupby("cluster_id"):
        center_lat = group["latitude"].mean()
        center_lng = group["longitude"].mean()
        center_coord = (center_lat, center_lng)

        categories = {}
        
        for cat in all_categories:
            cat_group = group[group["category"] == cat]
            current_count = len(cat_group)
            cat_target = target_per_cat[cat]
            
            items = []
            
            if current_count < cat_target:
                # ì „ì²´ ë°ì´í„°ì—ì„œ í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ì°¾ê¸°
                all_cat_places = df[df["category"] == cat].copy()
                
                # ì¤‘ì‹¬ìœ¼ë¡œë¶€í„° ê±°ë¦¬ ê³„ì‚°
                all_cat_places["distance_to_center"] = all_cat_places.apply(
                    lambda r: haversine_km(
                        center_coord,
                        (r["latitude"], r["longitude"])
                    ), axis=1
                )
                
                # ê°€ì¥ ê°€ê¹Œìš´ cat_targetê°œ ì„ íƒ
                nearest = all_cat_places.nsmallest(cat_target, "distance_to_center")
                
                for idx, row in nearest.iterrows():
                    items.append({
                        "id": int(row["id"]) if "id" in row and not pd.isna(row["id"]) else idx,
                        "name": row["name"],
                        "final_score": round(np.random.uniform(0.5, 0.9), 2),
                        "distance_from_center": round(row["distance_to_center"], 2)
                    })
                
                added = len(items) - current_count
                if added > 0:
                    print(f"  âœ… C{cid} {cat}: {current_count}ê°œ â†’ {len(items)}ê°œ (ê°€ê¹Œìš´ ê³³ {added}ê°œ ì¶”ê°€)")
            else:
                # ì´ë¯¸ ì¶©ë¶„í•˜ë©´ í˜„ì¬ í´ëŸ¬ìŠ¤í„°ì—ì„œë§Œ
                for idx, row in cat_group.head(cat_target).iterrows():
                    distance = haversine_km(center_coord, (row["latitude"], row["longitude"]))
                    items.append({
                        "id": int(row["id"]) if "id" in row and not pd.isna(row["id"]) else idx,
                        "name": row["name"],
                        "final_score": round(np.random.uniform(0.5, 0.9), 2),
                        "distance_from_center": round(distance, 2)
                    })
            
            categories[cat] = items

        hotzone = {
            "cluster_id": int(cid),
            "center_lat": round(center_lat, 6),
            "center_lng": round(center_lng, 6),
            "hotzone_score": round(
                min(1.0, len(group) / 50 + sum(len(v) for v in categories.values()) / 100), 2
            ),
            "category_diversity": sum(len(v) > 0 for v in categories.values()),
            "total_places": sum(len(v) for v in categories.values()),
            "original_cluster_size": len(group),
            "categories": categories
        }
        hotzones.append(hotzone)
    
    return {"hotzones": hotzones}


# ========================================
# ğŸ¨ ì‹œê°í™”
# ========================================
def visualize_clusters(df, save_path, title):
    plt.figure(figsize=(10, 8))
    clusters = sorted(df["cluster_id"].unique(), key=int)
    colors = plt.cm.tab20(np.linspace(0, 1, len(clusters)))

    for color, cid in zip(colors, clusters):
        sub = df[df["cluster_id"] == cid]
        plt.scatter(sub["longitude"], sub["latitude"], s=25, color=color, alpha=0.8, label=f"C{cid}")

    plt.xlabel("Longitude (ê²½ë„)")
    plt.ylabel("Latitude (ìœ„ë„)")
    plt.title(title)
    plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=7)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig(save_path, dpi=200)
    print(f"âœ… ì‹œê°í™” ì €ì¥ ì™„ë£Œ: {save_path}")
    plt.close()


# ========================================
# ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë³´ê³ ì„œ
# ========================================
def print_cluster_report(df, min_acceptable=3, min_size=15, ideal_per_cat=10, max_size=70):
    all_categories = ["Accommodation", "Cafe", "Restaurant", "Attraction"]
    print("\n" + "="*70)
    print("ğŸ“Š í´ëŸ¬ìŠ¤í„° í’ˆì§ˆ ë³´ê³ ì„œ")
    print("="*70)
    
    acceptable_count = 0
    ideal_count = 0
    critical_count = 0
    oversized_count = 0
    
    for cid, group in df.groupby("cluster_id"):
        cat_counts = group["category"].value_counts().to_dict()
        counts = {cat: cat_counts.get(cat, 0) for cat in all_categories}
        total = len(group)
        diversity = sum(1 for count in counts.values() if count > 0)
        
        is_too_small = total < min_size
        has_insufficient = any(count < min_acceptable for count in counts.values())
        is_ideal = all(count >= ideal_per_cat for count in counts.values())
        is_oversized = total > max_size
        
        if is_too_small or has_insufficient:
            status = "ğŸ”´ ê¸´ê¸‰"
            critical_count += 1
        elif is_ideal:
            status = "âœ… ì´ìƒì "
            ideal_count += 1
            acceptable_count += 1
        else:
            status = "âš ï¸ í—ˆìš©"
            acceptable_count += 1
        
        if is_oversized:
            status += " ğŸ”´ ì´ˆê³¼"
            oversized_count += 1
        
        print(f"\nğŸ·ï¸ í´ëŸ¬ìŠ¤í„° {cid} [{status}] (ì´ {total}ê°œ)")
        print(f"  ì¹´í…Œê³ ë¦¬ ë‹¤ì–‘ì„±: {diversity}/4")
        
        if is_too_small:
            print(f"  ğŸ”´ í¬ê¸° ë¶€ì¡±: ìµœì†Œ {min_size}ê°œ í•„ìš” (í˜„ì¬ {total}ê°œ)")
        
        for cat in all_categories:
            count = counts[cat]
            if count < min_acceptable:
                mark = "ğŸ”´"
            elif count >= ideal_per_cat:
                mark = "âœ“"
            else:
                mark = "âš "
            print(f"  {mark} {cat}: {count}ê°œ")
        
        if is_oversized:
            print(f"  ğŸ”´ í¬ê¸° ì´ˆê³¼: {max_size}ê°œ ì œí•œì„ {total - max_size}ê°œ ì´ˆê³¼!")
    
    print("\n" + "="*70)
    print(f"âœ… ì´ìƒì  í´ëŸ¬ìŠ¤í„°: {ideal_count}ê°œ")
    print(f"âš ï¸ í—ˆìš© í´ëŸ¬ìŠ¤í„°: {acceptable_count - ideal_count}ê°œ")
    print(f"ğŸ”´ ê¸´ê¸‰ ë³‘í•© í•„ìš”: {critical_count}ê°œ")
    if oversized_count > 0:
        print(f"ğŸ”´ í¬ê¸° ì´ˆê³¼ í´ëŸ¬ìŠ¤í„°: {oversized_count}ê°œ")
    print("="*70)


# ========================================
# ğŸš€ ì‹¤í–‰
# ========================================
if __name__ == "__main__":
    base_dir = r"C:\Users\changjin\workspace\lab\pln"
    data_path = os.path.join(base_dir, "data_set", "clustering_category_combine.csv")
    out_dir = os.path.join(base_dir, "clustering")
    os.makedirs(out_dir, exist_ok=True)

    df = pd.read_csv(data_path)
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({len(df)}ê°œ ì¥ì†Œ)")

    # 1ï¸âƒ£ ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ Greedy í´ëŸ¬ìŠ¤í„°ë§
    print("\n" + "="*70)
    print("ğŸ“ 1ë‹¨ê³„: ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ Greedy í´ëŸ¬ìŠ¤í„°ë§ ì‹œì‘")
    print("="*70)
    clustered = greedy_cluster(
        df, 
        target_cluster_size=50,
        base_radius=0.8,
        min_per_cat=3
    )
    print(f"\nğŸ“Š 1ì°¨ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(clustered['cluster_id'].unique())}")

    # 2ï¸âƒ£ ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„°ë¼ë¦¬ë§Œ ë³‘í•©
    merged = merge_unsatisfied_clusters(
        clustered, 
        min_per_cat=3,
        ideal_per_cat=10,
        min_cluster_size=15,
        max_cluster_size=70,
        merge_radius=8.0,
        max_iterations=15
    )
    print(f"ğŸ“Š ë³‘í•© í›„ ìµœì¢… í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(merged['cluster_id'].unique())}")

    # 3ï¸âƒ£ í’ˆì§ˆ ë³´ê³ ì„œ ì¶œë ¥
    print_cluster_report(merged, min_acceptable=3, min_size=15, ideal_per_cat=10, max_size=70)

    # 4ï¸âƒ£ ì‹œê°í™”
    visualize_clusters(clustered, os.path.join(out_dir, "step1_greedy.png"), "ğŸ“ 1ë‹¨ê³„ Greedy í´ëŸ¬ìŠ¤í„°ë§")
    visualize_clusters(merged, os.path.join(out_dir, "step2_merged.png"), "ğŸ“ 2ë‹¨ê³„ ë¶ˆë§Œì¡± í´ëŸ¬ìŠ¤í„° ë³‘í•©")

    # 5ï¸âƒ£ JSON ì €ì¥
    print("\n" + "="*70)
    print("ğŸ“¦ 5ë‹¨ê³„: Hotzone JSON ìƒì„± (ìˆ™ì†Œ:30ê°œ, ì‹ë‹¹:25ê°œ, ì¹´í˜:10ê°œ, ê´€ê´‘:10ê°œ)")
    print("="*70)
    hotzones = build_hotzones(merged)  # ìë™ìœ¼ë¡œ ë°ì´í„° ë¶„í¬ì— ë§ê²Œ ê³„ì‚°
    json_path = os.path.join(out_dir, "greedy_hotzones_merged.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(hotzones, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ Hotzones JSON ì €ì¥ ì™„ë£Œ: {json_path}")
    print(f"ğŸ“Š ì´ {len(hotzones['hotzones'])}ê°œ Hotzone ìƒì„±")

    print("\nâœ¨ ë¹„ìœ¨ ê¸°ë°˜ ë™ì  í• ë‹¹ + í¬ê¸°/ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ë³‘í•© ì™„ë£Œ!")