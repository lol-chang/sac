# -*- coding: utf-8 -*-
"""
ë„¤ì´ë²„ ì§€ë„ placeId í¬ë¡¤ëŸ¬ (ëª©ë¡ ì „ì²´ + 'ê°•ë¦‰' ì£¼ì†Œ í•„í„° + í–‰ë‹¨ìœ„ ì €ì¥)

- ê²€ìƒ‰ ê²°ê³¼ê°€ ë‹¨ì¼ ë¼ìš°íŒ…ì´ë©´ entryIframeì—ì„œ ì£¼ì†Œë¥¼ ì°¾ì•„ 'ê°•ë¦‰' í¬í•¨ ì‹œì—ë§Œ ì €ì¥
- ëª©ë¡ í™”ë©´ì´ë©´ li ì „ë¶€ í›‘ì–´ ì£¼ì†Œ span í…ìŠ¤íŠ¸ì— 'ê°•ë¦‰' í¬í•¨ëœ í•­ëª©ë§Œ í´ë¦­ â†’ entryIframe URLì—ì„œ placeId ì¶”ì¶œ
- ì €ì¥ ìŠ¤í‚¤ë§ˆ: [no, store_name, store_url_naver] ë¥¼ í•œ ì¤„ì”© append
"""

import os, re, time, urllib.parse
import pandas as pd
from openpyxl import Workbook, load_workbook
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, StaleElementReferenceException

# ===== ì‚¬ìš©ì ì„¤ì • =====
INPUT_PATH   = "ê°•ë¦‰ì‹œ_ì¼ë°˜ìŒì‹ì  í˜„í™©.csv"  # ì…ë ¥ CSV
OUTPUT_PATH  = "naver_placeid_list.xlsx"     # ê²°ê³¼ ì—‘ì…€
NAME_COL     = "ì—…ì†Œëª…"                        # CSVì—ì„œ ìƒí˜¸ëª… ì»¬ëŸ¼ëª…
CITY_FILTER  = "ê°•ë¦‰"                          # ì£¼ì†Œì— ì´ ë¬¸ìì—´ì´ í¬í•¨ë˜ì–´ì•¼ ì €ì¥

# ===== ê³µí†µ ìœ í‹¸ =====
def read_korean_csv(path: str) -> pd.DataFrame:
    for enc in ("utf-8-sig", "cp949", "ms949", "euc-kr"):
        try:
            return pd.read_csv(path, encoding=enc)
        except UnicodeDecodeError:
            continue
    return pd.read_csv(path, encoding="utf-8", errors="ignore")

def build_review_url(place_id: str) -> str:
    # m.place ë°©ë¬¸ì ë¦¬ë·° URL
    return f"https://m.place.naver.com/restaurant/{place_id}/review/visitor?entry=ple&reviewSort=recent"

def extract_place_id_from_url(url: str) -> str | None:
    m = re.search(r"/place/(\d+)", url)
    if m:
        return m.group(1)
    m = re.search(r"[?&#]id=(\d+)", url)
    return m.group(1) if m else None

# ===== ì €ì¥: í–‰ ë‹¨ìœ„ append (í—¤ë” ìœ ì§€) =====
def append_single_row(path: str, no: int, store_name: str, store_url_naver: str):
    if not os.path.exists(path):
        wb = Workbook()
        ws = wb.active
        ws.title = "Sheet1"
        ws.append(["no", "store_name", "store_url_naver"])
        wb.save(path)
        print(f"ğŸ“„ ìƒˆ íŒŒì¼ ìƒì„± + í—¤ë” ê¸°ë¡: {path}")

    wb = load_workbook(path)
    ws = wb.active
    ws.append([no, store_name, store_url_naver])
    wb.save(path)
    print(f"ğŸ“ ì €ì¥ì™„ë£Œ | no={no}, store_name='{store_name}'")

# ===== í”„ë ˆì„ í—¬í¼ =====
def _switch_to_search_iframe(driver, wait):
    driver.switch_to.default_content()
    wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "searchIframe")))

def _switch_to_entry_iframe(driver, wait):
    driver.switch_to.default_content()
    wait.until(EC.frame_to_be_available_and_switch_to_it((By.ID, "entryIframe")))

# ===== ëª©ë¡ ìŠ¤í¬ë¡¤/ì…€ë ‰í„° =====
def _scroll_all_in_list(driver):
    # ë¬´í•œìŠ¤í¬ë¡¤ ì»¨í…Œì´ë„ˆ í›„ë³´
    candidates = [
        'div#_pcmap_list_scroll_container',
        'div#_pcmap_list_scroll_container div.api_scroller',
        'div[id$="_pcmap_list_scroll_container"]',
        'div#_pcmap_list_scroll_container ul'
    ]
    container = None
    for sel in candidates:
        try:
            container = driver.find_element(By.CSS_SELECTOR, sel)
            break
        except:
            pass
    if container is None:
        return
    prev_h = 0; stall = 0
    while True:
        driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight;", container)
        time.sleep(0.5)
        h = driver.execute_script("return arguments[0].scrollHeight;", container)
        stall = stall + 1 if h == prev_h else 0
        if stall >= 3:
            break
        prev_h = h

def _find_list_items(driver):
    # li ì„ íƒ (ë³€ë™ ëŒ€ë¹„ ë‹¤ì¤‘ ì…€ë ‰í„°)
    li_selectors = [
        'div#_pcmap_list_scroll_container ul > li',
        'ul > li.UEzoS',
        'ul > li'
    ]
    for sel in li_selectors:
        lis = driver.find_elements(By.CSS_SELECTOR, sel)
        if lis:
            return lis
    return []

def _find_title_anchor(li):
    a_selectors = [
        'a.place_bluelink',
        'a.tit_name',
        'a[href*="/place/"]',
        'a[role="button"]',
        'a'
    ]
    for sel in a_selectors:
        try:
            return li.find_element(By.CSS_SELECTOR, sel)
        except:
            continue
    return None

# ===== ì£¼ì†Œ ì¶”ì¶œ =====
def _extract_address_text_from_li(li) -> str:
    """
    ëª©ë¡ li ë‚´ë¶€ì˜ 'ì£¼ì†Œ' í…ìŠ¤íŠ¸ë¥¼ ì°¾ì•„ ë°˜í™˜.
    - ìŠ¤ìƒ· ê¸°ì¤€ í›„ë³´: span.Pb4bU
    - í´ë˜ìŠ¤ ìˆ˜ì‹œ ê°œí¸ ëŒ€ë¹„ í´ë°± í¬í•¨
    """
    # í›„ë³´ ìš°ì„ 
    for sel in ['span.Pb4bU', 'span[class*="addr"]', 'div[class*="address"] span']:
        spans = li.find_elements(By.CSS_SELECTOR, sel)
        for sp in spans:
            t = sp.text.strip()
            if t:
                return t
    # CITY_FILTERê°€ ë“¤ì–´ê°„ span ì°¾ê¸°
    for sp in li.find_elements(By.CSS_SELECTOR, "span"):
        t = sp.text.strip()
        if t and CITY_FILTER in t:
            return t
    # ì‹œ/êµ°/êµ¬ í¬í•¨ ê·¸ëŸ´ì‹¸í•œ í…ìŠ¤íŠ¸
    for sp in li.find_elements(By.CSS_SELECTOR, "span"):
        t = sp.text.strip()
        if any(token in t for token in ("ì‹œ", "êµ°", "êµ¬")) and len(t) >= 4:
            return t
    return ""

def _extract_address_text_from_entry(driver) -> str:
    """
    ìƒì„¸(entryIframe)ì—ì„œ ì£¼ì†Œ í…ìŠ¤íŠ¸ ì¶”ì¶œ(ë‹¨ì¼ ë¼ìš°íŒ…ìš©).
    êµ¬ì¡°ê°€ ì¢…ì¢… ë°”ë€Œë¯€ë¡œ ë„“ê²Œ íƒìƒ‰ í›„ CITY_FILTER í¬í•¨ í…ìŠ¤íŠ¸ë¥¼ ìš°ì„  ë°˜í™˜.
    """
    # ì£¼ì†Œ ì˜ì—­ì— í”í•œ íŒ¨í„´ë“¤
    candidates = [
        'span[class*="addr"]',
        'div[class*="address"]',
        'a[href^="https://map.naver.com/"] span',
        'div:has(> span) span'
    ]
    # 1) í›„ë³´ ì…€ë ‰í„° ìš°ì„ 
    for sel in candidates:
        try:
            elems = driver.find_elements(By.CSS_SELECTOR, sel)
            texts = [e.text.strip() for e in elems if e.text.strip()]
            for t in texts:
                if CITY_FILTER in t:
                    return t
        except:
            pass
    # 2) ì „ì²´ span ì¤‘ì—ì„œ CITY_FILTER í¬í•¨
    try:
        for sp in driver.find_elements(By.CSS_SELECTOR, "span"):
            t = sp.text.strip()
            if t and CITY_FILTER in t:
                return t
    except:
        pass
    return ""

# ===== í•µì‹¬: ê²€ìƒ‰ + í•„í„° + ìˆ˜ì§‘ =====
def search_and_collect_filtered(driver, keyword: str, wait: WebDriverWait, settle_sec: float = 1.0):
    """
    ë°˜í™˜: list[tuple(display_name, place_id, address)]
    """
    encoded = urllib.parse.quote(keyword, safe="")
    url = f"https://map.naver.com/p/search/{encoded}"
    driver.get(url)
    time.sleep(settle_sec)

    # 1) ë‹¨ì¼ ë¼ìš°íŒ… ê°ì§€
    try:
        WebDriverWait(driver, 5).until(lambda d: re.search(r"/place/\d+", d.current_url) is not None)
    except Exception:
        pass

    pid = extract_place_id_from_url(driver.current_url)
    if pid:
        # ë‹¨ì¼ ìƒì„¸ â†’ entryIframe ì§„ì…í•´ì„œ ì£¼ì†Œ í™•ì¸
        try:
            _switch_to_entry_iframe(driver, wait)
            addr = _extract_address_text_from_entry(driver)
        except TimeoutException:
            addr = ""
        if CITY_FILTER in addr:
            print(f"âœ… ë‹¨ì¼ê²°ê³¼ ì €ì¥ â€¢ {keyword} [{addr}] â†’ {pid}")
            return [(keyword, pid, addr)]
        else:
            print(f"â›” ë‹¨ì¼ê²°ê³¼ ìŠ¤í‚µ(ì£¼ì†Œ ë¶ˆì¼ì¹˜) â€¢ {keyword} [{addr}]")
            return []

    # 2) ëª©ë¡ ì²˜ë¦¬
    results = []
    try:
        _switch_to_search_iframe(driver, wait)
    except TimeoutException:
        print("âš ï¸ searchIframe ì§„ì… ì‹¤íŒ¨")
        return results

    _scroll_all_in_list(driver)
    lis = _find_list_items(driver)
    if not lis:
        print("âš ï¸ ëª©ë¡(li) íƒìƒ‰ ì‹¤íŒ¨")
        return results

    print(f"ğŸ“œ í›„ë³´ {len(lis)}ê°œ ë°œê²¬ (í•„í„°: '{CITY_FILTER}')")

    for idx in range(len(lis)):
        try:
            # stale ë°©ì§€: ë§¤íšŒ ì¬ì¡°íšŒ
            lis = _find_list_items(driver)
            li = lis[idx]

            addr = _extract_address_text_from_li(li)
            if CITY_FILTER not in addr:
                print(f"  â€¢ #{idx+1} ì£¼ì†Œ ë¯¸ì¼ì¹˜ â†’ ìŠ¤í‚µ | addr='{addr}'")
                continue

            # í‘œì‹œ ì´ë¦„(ê°€ëŠ¥í•˜ë©´ li ì²« ì¤„)
            try:
                display_name = li.text.split("\n", 1)[0].strip()
            except:
                display_name = keyword

            a = _find_title_anchor(li)
            if a is None:
                print(f"  â€¢ #{idx+1} ë§í¬ ë¯¸ë°œê²¬ â†’ ìŠ¤í‚µ")
                continue

            # í´ë¦­í•˜ì—¬ ìƒì„¸ ì§„ì… â†’ entryIframe URLì—ì„œ id ì¶”ì¶œ
            driver.execute_script("arguments[0].scrollIntoView({block:'center'});", li)
            time.sleep(0.2)
            a.click()

            _switch_to_entry_iframe(driver, wait)
            WebDriverWait(driver, 8).until(lambda d: re.search(r"/place/\d+", d.current_url) is not None)
            current = driver.current_url
            pid = extract_place_id_from_url(current)
            if not pid:
                print(f"  â€¢ #{idx+1} {display_name} [{addr}] â†’ placeId íŒŒì‹± ì‹¤íŒ¨")
                _switch_to_search_iframe(driver, wait)
                continue

            results.append((display_name, pid, addr))
            print(f"  âœ… ì €ì¥ëŒ€ìƒ â€¢ #{idx+1} {display_name} [{addr}] â†’ {pid}")

            _switch_to_search_iframe(driver, wait)
            time.sleep(0.2)

        except (TimeoutException, StaleElementReferenceException) as e:
            try:
                _switch_to_search_iframe(driver, wait)
            except:
                pass
            print(f"  â€¢ #{idx+1} ì˜¤ë¥˜: {e} â†’ ê³„ì†")
            continue
        except Exception as e:
            print(f"  â€¢ #{idx+1} ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜: {e} â†’ ê³„ì†")
            try:
                _switch_to_search_iframe(driver, wait)
            except:
                pass
            continue

    return results

# ===== ë©”ì¸ =====
def main():
    print("ğŸš€ ì‹œì‘")
    df = read_korean_csv(INPUT_PATH)
    print(f"ğŸ“‹ ëŒ€ìƒ {len(df)}ê±´")

    options = webdriver.ChromeOptions()
    options.add_argument("--start-maximized")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    driver = webdriver.Chrome(options=options)
    wait = WebDriverWait(driver, 15)

    no = 1
    saved = skipped = 0

    for idx, row in df.iterrows():
        keyword = str(row.get(NAME_COL, "")).strip()
        if not keyword:
            continue

        print(f"\n{'='*60}\nğŸ” [{idx+1}/{len(df)}] ê²€ìƒ‰: {keyword}")
        items = search_and_collect_filtered(driver, keyword, wait, settle_sec=2.0)

        if not items:
            print(f"âŒ ì €ì¥ ì—†ìŒ: {keyword}")
            skipped += 1
            continue

        for display_name, pid, addr in items:
            review_url = build_review_url(pid)
            append_single_row(OUTPUT_PATH, no, display_name, review_url)
            print(f"ğŸ¯ ì €ì¥ â€¢ no={no} | {display_name} [{addr}] â†’ {pid} | {review_url}")
            no += 1
            saved += 1
            time.sleep(0.2)

    driver.quit()
    print(f"\nğŸ‰ ì™„ë£Œ: ì €ì¥ {saved}ê±´, ìŠ¤í‚µ {skipped}ê±´")
    print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {OUTPUT_PATH}")

if __name__ == "__main__":
    main()
